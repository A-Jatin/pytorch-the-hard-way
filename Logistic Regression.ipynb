{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "import random\n",
    "import numpy as np\n",
    "import torch as T\n",
    "from utils.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Iterable\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# set default device based on CUDA's availability\n",
    "device = 'cuda' if T.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "cifar10_dir = 'utils/datasets/cifar-10-batches-py'\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "class1 = (y_train == 1)\n",
    "class0 = (y_train == 0)\n",
    "classes = class0.astype(np.uint8) | class1.astype(np.uint8)\n",
    "\n",
    "class1_ = (y_test == 1)\n",
    "class0_ = (y_test == 0)\n",
    "classes_ = class0_.astype(np.uint8) | class1_.astype(np.uint8)\n",
    "\n",
    "# Subsample the data for more efficient code execution in this exercise\n",
    "X_train = X_train[classes == True]\n",
    "y_train = y_train[classes == True]\n",
    "\n",
    "X_test = X_test[classes_ == True]\n",
    "y_test = y_test[classes_ == True]\n",
    "\n",
    "y_train = np.expand_dims(y_train, 0)\n",
    "y_test = np.expand_dims(y_test, 0)\n",
    "\n",
    "# Normalize images\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train = np.rollaxis(X_train.reshape(-1, 32 * 32 * 3), 1, 0)\n",
    "X_test = np.rollaxis(X_test.reshape(-1, 32 * 32 * 3), 1, 0)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "**Mathematical expression of the algorithm**\n",
    "\n",
    "For one example $x^{(i)}$\n",
    "$$z^{(i)} = w^T x^{(i)} + b$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$\n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)})  \\log(1-a^{(i)})$$\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's implement the sigmoid function and visualize the result to check the implementation.\n",
    "\n",
    "Remember a sigmoid function is defined as:\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Hint: [click](https://pytorch.org/docs/stable/torch.html#torch.exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: T.Tensor) -> T.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    \"\"\"\n",
    "    #### YOUR CODE STARTS HERE ####\n",
    "    s = ...\n",
    "    #### YOUR CODE ENDS HERE ####\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = T.tensor(9.2, device=device)\n",
    "print('sigmoid(9.2) = ', sigmoid(T.tensor(9.2)))\n",
    "\n",
    "tensor = T.tensor(0.0, device=device)\n",
    "print('sigmoid(0) = ', sigmoid(T.tensor(0.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "sigmoid(9.2) =  tensor(0.9999)\n",
    "sigmoid(0) =  tensor(0.5000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = T.arange(-10, 10, 0.1, device=device)\n",
    "s = sigmoid(z)\n",
    "# print(type(z), type(s))\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(z.cpu().numpy(), s.cpu().numpy())\n",
    "plt.title('Sigmoid Function $\\sigma$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim: int, device: str) -> Iterable[T.Tensor]:\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    dim -- size of the w tensor\n",
    "    \n",
    "    Returns:\n",
    "    W -- initialized tensor of shape (dim, 1) for weights\n",
    "    b -- initialized tensor for the bias (scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    #### YOUR CODE STARTS HERE ####\n",
    "    W = ...\n",
    "    b = ...\n",
    "    #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "W, b = initialize_with_zeros(dim, device=device)\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "tensor([[ 0.],\n",
    "        [ 0.]])\n",
    "tensor(0.)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the propagation logistic regression is defined as:\n",
    "$$Z = W^T X + b$$\n",
    "$$\\hat{Y} = A = sigmoid(Z)$$\n",
    "$$\\mathcal{L}(A, Y) = \\frac{1}{m}(- Y  \\log(A) - (1-Y)  \\log(1-A))$$\n",
    "$$\\frac{\\delta J}{\\delta W} = \\frac{1}{m}(X(A-Y)^T)$$\n",
    "$$\\frac{\\delta J}{\\delta b} = \\frac{1}{m}(A-Y)^T$$\n",
    "\n",
    "Hint: [click](https://pytorch.org/docs/stable/torch.html#torch.transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(W, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    #### YOUR CODE STARTS HERE ####\n",
    "    Z = ...\n",
    "    A = ...  # compute activation\n",
    "    cost = ...  # compute cost\n",
    "    #### YOUR CODE ENDS HERE ####\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    #### YOUR CODE STARTS HERE ####\n",
    "    dW = ...\n",
    "    db = ...\n",
    "    #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "    grads = {'dW': dW,\n",
    "             'db': db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = T.tensor([[1.],[2.]], device=device), T.tensor(2., device=device), \n",
    "X, Y = T.tensor([[1.,2.,-1.],[3.,4.,-3.2]], device=device), T.tensor([[1.,0.,1.]], device=device)\n",
    "grads, cost = propagate(W, b, X, Y)\n",
    "print('dW =', grads['dW'])\n",
    "print('db =', grads['db'])\n",
    "print('cost =', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "dW = tensor([[ 0.9985], [ 2.3951]])\n",
    "db = tensor(1.00000e-03 * 1.4556)\n",
    "cost = tensor(5.7986)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(W, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        grads, cost = ...\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dW = grads['dW']\n",
    "        db = grads['db']\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        W = ...\n",
    "        b = ...\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost.cpu().numpy())\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print ('Cost after iteration %i: %f' % (i, cost.cpu().numpy()))\n",
    "    \n",
    "    params = {'W': W,\n",
    "              'b': b}\n",
    "    \n",
    "    grads = {'dW': dW,\n",
    "             'db': db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, grads, costs = optimize(W, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n",
    "\n",
    "print ('W =', params['W'])\n",
    "print ('b =', params['b'])\n",
    "print ('dW =', grads['dW'])\n",
    "print ('db =', grads['db'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "W = tensor([[ 0.1903], [ 0.1226]])\n",
    "b = tensor(1.9254)\n",
    "dW = tensor([[ 0.6775], [ 1.4163]])\n",
    "db = tensor(0.2192)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W, b, X):\n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    WT = T.transpose(W, 0, 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    #### YOUR CODE STARTS HERE ####\n",
    "    A = ...\n",
    "    #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(W, b, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "tensor([[ 0.9999,  1.0000,  0.0045]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, device='cpu', num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \"\"\"\n",
    "    \n",
    "    #### YOUR CODE STARTS HERE ####\n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    W, b = ...\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = ...\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    W = parameters['W']\n",
    "    b = parameters['b']\n",
    "    #### YOUR CODE ENDS HERE ####\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = (predict(W, b, X_test).cpu().numpy() > 0.5).astype(np.uint8)\n",
    "    Y_prediction_train = (predict(W, b, X_train).cpu().numpy() > 0.5).astype(np.uint8)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print('train accuracy: {} %'.format(100 - np.mean(np.abs(Y_prediction_train - Y_train.cpu().numpy())) * 100))\n",
    "    print('test accuracy: {} %'.format(100 - np.mean(np.abs(Y_prediction_test - Y_test.cpu().numpy())) * 100))\n",
    "\n",
    "    \n",
    "    d = {'costs': costs,\n",
    "         'Y_prediction_test': Y_prediction_test, \n",
    "         'Y_prediction_train' : Y_prediction_train, \n",
    "         'W' : W, \n",
    "         'b' : b,\n",
    "         'learning_rate' : learning_rate,\n",
    "         'num_iterations': num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cuda':\n",
    "    X_train_T = T.from_numpy(X_train).float().cuda()\n",
    "    y_train_T = T.from_numpy(y_train).float().cuda()\n",
    "    X_test_T = T.from_numpy(X_test).float().cuda()\n",
    "    y_test_T = T.from_numpy(y_test).float().cuda()\n",
    "else:\n",
    "    X_train_T = T.from_numpy(X_train).float()\n",
    "    y_train_T = T.from_numpy(y_train).float()\n",
    "    X_test_T = T.from_numpy(X_test).float()\n",
    "    y_test_T = T.from_numpy(y_test).float()\n",
    "    \n",
    "X_train_T.size(), y_train_T.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model(X_train_T, y_train_T, X_test_T, y_test_T, device=device, num_iterations=1500, learning_rate=0.01, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title('Learning rate =' + str(d['learning_rate']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    print ('learning rate is: ' + str(i))\n",
    "    models[str(i)] = model(X_train_T, y_train_T, X_test_T, y_test_T, device=device, num_iterations=15000, learning_rate=i, print_cost=False)\n",
    "    print ('\\n' + '-------------------------------------------------------' + '\\n')\n",
    "\n",
    "for i in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(i)]['costs']), label= str(models[str(i)]['learning_rate']))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (hundreds)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "- Different learning rates give different costs and thus different predictions results.\n",
    "- If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost).\n",
    "- A lower cost doesn't mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.\n",
    "- In deep learning, we usually recommend that you:\n",
    "    - Choose the learning rate that better minimizes the cost function.\n",
    "    - If your model overfits, use other techniques to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
